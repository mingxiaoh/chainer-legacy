

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Define your own function &mdash; Chainer 2.0.0b1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/modified_theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Chainer 2.0.0b1 documentation" href="../index.html"/>
        <link rel="up" title="Chainer Tutorial" href="index.html"/>
        <link rel="next" title="Type check" href="type_check.html"/>
        <link rel="prev" title="Using GPU(s) in Chainer" href="gpu.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Chainer
          

          
          </a>

          
            
            
              <div class="version">
                2.0.0b1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Chainer Tutorial</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="basic.html">Introduction to Chainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="recurrentnet.html">Recurrent Nets and their Computational Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu.html">Using GPU(s) in Chainer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Define your own function</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#differentiable-functions">Differentiable Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#unified-forward-backward-methods-with-numpy-cupy-functions">Unified forward/backward methods with NumPy/CuPy functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#write-an-elementwise-kernel-function">Write an Elementwise Kernel Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="#write-a-function-with-training-test-mode">Write a function with training/test mode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#links-that-wrap-functions">Links that wrap functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing-function">Testing Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="type_check.html">Type check</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">Chainer Reference Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Chainer Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compatibility.html">API Compatibility Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tips.html">Tips and FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../comparison.html">Comparison with Other Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Chainer</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Chainer Tutorial</a> &raquo;</li>
        
      <li>Define your own function</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/function.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="define-your-own-function">
<h1>Define your own function<a class="headerlink" href="#define-your-own-function" title="Permalink to this headline">¶</a></h1>
<p>In this section, you will learn about the following things:</p>
<ul class="simple">
<li>How to define a function on variables</li>
<li>Useful tools to write a function using a GPU</li>
<li>How to test the function definition</li>
</ul>
<p>After reading this section, you will be able to:</p>
<ul class="simple">
<li>Write your own functions</li>
<li>Define simple kernels in the function definition</li>
</ul>
<div class="section" id="differentiable-functions">
<h2>Differentiable Functions<a class="headerlink" href="#differentiable-functions" title="Permalink to this headline">¶</a></h2>
<p>Chainer provides a collection of functions in the <a class="reference internal" href="../reference/functions.html#module-chainer.functions" title="chainer.functions"><code class="xref py py-mod docutils literal"><span class="pre">functions</span></code></a> module.
It covers typical use cases in deep learning, so many existing works can be implemented with them.
On the other hand, deep learning is evolving rapidly and we cannot cover all possible functions to define unseen architectures.
So it is important to learn how to define your own functions.</p>
<p>First, suppose we want to define an elementwise function <span class="math">\(f(x, y, z) = x * y + z\)</span>.
While it is possible to implement this equation using a combination of the <code class="docutils literal"><span class="pre">*</span></code> and <code class="docutils literal"><span class="pre">+</span></code> functions,
defining it as a single function may reduce memory consumption, so it is not <em>only</em> a toy example.
Here we call this function <em>MulAdd</em>.</p>
<p>Let’s start with defining MulAdd working on the CPU.
Any function must inherit the <a class="reference internal" href="../reference/core/function.html#chainer.Function" title="chainer.Function"><code class="xref py py-class docutils literal"><span class="pre">Function</span></code></a> class.
The skeleton of a function looks like:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># do forward computation on CPU</span>
        <span class="k">return</span> <span class="n">some_tuple</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="c1"># do backward computation on CPU</span>
        <span class="k">return</span> <span class="n">some_tuple</span>
</pre></div>
</div>
<p>We must implement <a class="reference internal" href="../reference/core/function.html#chainer.Function.forward_cpu" title="chainer.Function.forward_cpu"><code class="xref py py-meth docutils literal"><span class="pre">forward_cpu()</span></code></a> and <a class="reference internal" href="../reference/core/function.html#chainer.Function.backward_cpu" title="chainer.Function.backward_cpu"><code class="xref py py-meth docutils literal"><span class="pre">backward_cpu()</span></code></a> methods.
The non-self arguments of these functions are tuples of array(s), and these functions must return a tuple of array(s).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Be careful to return a tuple of arrays even if you have just one array to return.</p>
</div>
<p>MulAdd is simple and implemented as follows</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gw</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gz</span> <span class="o">=</span> <span class="n">gw</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
</pre></div>
</div>
<p>As per the warning above, the <code class="docutils literal"><span class="pre">forward_cpu</span></code> method returns a tuple of single element.
Note that all arrays appearing in CPU functions are <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>.
The forward function is straightforward:
It unpacks the input tuple, computes the output, and packs it into a tuple.
The backward function is a bit more complicated.
Recall the rule of differentiation of multiplication.
This example just implements the rule.
Look at the return values, the function just packs the gradient of each input in same order and returns them.</p>
<p>By just defining the core computation of forward and backward,
Function class provides a chaining logic on it (i.e. storing the
history of computation, etc.).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Assuming we implement a (forward) function <span class="math">\(y=f(x)\)</span> which takes as input the
vector <span class="math">\(x \in \mathbb{R}^n\)</span> and produces as output a vector
<span class="math">\(y \in \mathbb{R}^m\)</span>. Then the <code class="docutils literal"><span class="pre">backward</span></code> method has to compute</p>
<div class="math">
\[\lambda_i = \sum_{j=1}^m \frac{\partial y_j}{\partial x_i} \,
\gamma_j \,\, \text{for}\, i = 1 \dots n\]</div>
<p class="last">where <span class="math">\(\gamma\)</span> is the <code class="docutils literal"><span class="pre">grad_outputs</span></code>. Note, that the
resulting vector <span class="math">\(\lambda\)</span> must have the same shape as the arguments of the <code class="docutils literal"><span class="pre">forward</span></code> method.</p>
</div>
<p>Now let’s define the corresponding GPU methods.
You can easily predict that the methods we have to write are named <a class="reference internal" href="../reference/core/function.html#chainer.Function.forward_gpu" title="chainer.Function.forward_gpu"><code class="xref py py-meth docutils literal"><span class="pre">forward_gpu()</span></code></a> and <a class="reference internal" href="../reference/core/function.html#chainer.Function.backward_gpu" title="chainer.Function.backward_gpu"><code class="xref py py-meth docutils literal"><span class="pre">backward_gpu()</span></code></a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gw</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gz</span> <span class="o">=</span> <span class="n">gw</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
</pre></div>
</div>
<p>In GPU methods, arrays are of type <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code>.
We use arithmetic operators defined for this class.
These operators implement the basic elementwise arithmetics.</p>
<p>You may find that the definitions of GPU methods are exactly same as those of CPU methods.
In that case, we can reduce them to <a class="reference internal" href="../reference/core/function.html#chainer.Function.forward" title="chainer.Function.forward"><code class="xref py py-meth docutils literal"><span class="pre">forward()</span></code></a> and <a class="reference internal" href="../reference/core/function.html#chainer.Function.backward" title="chainer.Function.backward"><code class="xref py py-meth docutils literal"><span class="pre">backward()</span></code></a> methods</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">z</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gw</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">gw</span>
        <span class="n">gz</span> <span class="o">=</span> <span class="n">gw</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
</pre></div>
</div>
<p>Since the <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code> class implements many methods of <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>, we can write these unified methods in most cases.</p>
<p>The MulAdd function is used as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">MulAdd</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<p>It looks a bit ugly: we have to explicitly instantiate MulAdd before applying it to variables.
We also have to be careful that one instance of MulAdd must not be used multiple times, since it acts as a node in the computational graph.
In Chainer, we often define a thin wrapper Python function that hide the instantiation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">muladd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">MulAdd</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">muladd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="unified-forward-backward-methods-with-numpy-cupy-functions">
<h2>Unified forward/backward methods with NumPy/CuPy functions<a class="headerlink" href="#unified-forward-backward-methods-with-numpy-cupy-functions" title="Permalink to this headline">¶</a></h2>
<p>CuPy also implements many functions that are compatible to those of NumPy.
We can write unified forward/backward methods with them.
Consider that we want to write a backprop-able function <span class="math">\(f(x, y) = \exp(x) + \exp(y)\)</span>.
We name it <em>ExpAdd</em> here.
It can be written straight-forward as follows</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExpAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gz</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span>

    <span class="k">def</span> <span class="nf">forward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cupy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">cupy</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">cupy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">cupy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">cupy</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gz</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">cupy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">cupy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span>

<span class="k">def</span> <span class="nf">expadd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ExpAdd</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Here we used <code class="docutils literal"><span class="pre">cuda.cupy</span></code> instead of directly accessing <code class="xref py py-mod docutils literal"><span class="pre">cupy</span></code>.
This is because the <code class="docutils literal"><span class="pre">cupy</span></code> module cannot be imported if the CUDA is not installed.
In order to keep the implementation valid in non-CUDA environment, we have to defer the access to the <code class="docutils literal"><span class="pre">cupy</span></code> module.
Note that the <a class="reference internal" href="../reference/util/cuda.html#module-chainer.cuda" title="chainer.cuda"><code class="xref py py-mod docutils literal"><span class="pre">chainer.cuda</span></code></a> module can be imported even if the CUDA is not installed.
Of course, the module in such environment is almost useless, but if the interpreter does not run through the code accessing CUDA-dedicated functions, the code is still valid.</p>
</div>
<p>The CPU and GPU implementations are almost same, except that <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/index.html#module-numpy" title="(in NumPy v1.13)"><code class="xref py py-mod docutils literal"><span class="pre">numpy</span></code></a> is replaced by <code class="xref py py-mod docutils literal"><span class="pre">cupy</span></code> in GPU methods.
We can unify these functions using the <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.get_array_module" title="chainer.cuda.get_array_module"><code class="xref py py-func docutils literal"><span class="pre">cuda.get_array_module()</span></code></a> function.
This function accepts arbitrary number of arrays, and returns an appropriate module for them.
See the following code</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExpAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">xp</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_array_module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">xp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">xp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">xp</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_array_module</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gz</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">xp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">gy</span> <span class="o">=</span> <span class="n">gz</span> <span class="o">*</span> <span class="n">xp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span>

<span class="k">def</span> <span class="nf">expadd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">ExpAdd</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that this code works correctly even if CUDA is not installed in the environment.
If CUDA is not found, get_array_module function always returns <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/index.html#module-numpy" title="(in NumPy v1.13)"><code class="xref py py-mod docutils literal"><span class="pre">numpy</span></code></a>.
We often use the name <code class="docutils literal"><span class="pre">xp</span></code> for the variadic module name, which is analogous to the abbreviation <code class="docutils literal"><span class="pre">np</span></code> for NumPy and <code class="docutils literal"><span class="pre">cp</span></code> for CuPy.</p>
</div>
<div class="section" id="write-an-elementwise-kernel-function">
<h2>Write an Elementwise Kernel Function<a class="headerlink" href="#write-an-elementwise-kernel-function" title="Permalink to this headline">¶</a></h2>
<p>Let’s turn back to the MulAdd example.</p>
<p>The GPU implementation of MulAdd as shown above is already fast and parallelized on GPU cores.
However, it invokes two kernels during each of forward and backward computations.
It might hurt performance, since the intermediate temporary arrays are read and written by possibly different GPU cores, which consumes much bandwidth.
We can reduce the number of invocations by defining our own kernel.
It also reduce the memory consumption.</p>
<p>Most functions only require elementwise operations like MulAdd.
CuPy provides a useful tool to define elementwise kernels, the <code class="xref py py-class docutils literal"><span class="pre">cupy.elementwise.ElementwiseKernel</span></code> class, and Chainer wraps it by <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.elementwise" title="chainer.cuda.elementwise"><code class="xref py py-func docutils literal"><span class="pre">cuda.elementwise()</span></code></a> function.
Our MulAdd implementation can be improved as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cupy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">cupy</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">elementwise</span><span class="p">(</span>
            <span class="s1">&#39;float32 x, float32 y, float32 z&#39;</span><span class="p">,</span>
            <span class="s1">&#39;float32 w&#39;</span><span class="p">,</span>
            <span class="s1">&#39;w = x * y + z&#39;</span><span class="p">,</span>
            <span class="s1">&#39;muladd_fwd&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gw</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">elementwise</span><span class="p">(</span>
            <span class="s1">&#39;float32 x, float32 y, float32 gw&#39;</span><span class="p">,</span>
            <span class="s1">&#39;float32 gx, float32 gy&#39;</span><span class="p">,</span>
            <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">               gx = y * gw;</span>
<span class="sd">               gy = x * gw;</span>
<span class="sd">            &#39;&#39;&#39;</span><span class="p">,</span>
            <span class="s1">&#39;muladd_bwd&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gw</span><span class="p">)</span>

        <span class="n">gz</span> <span class="o">=</span> <span class="n">gw</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
</pre></div>
</div>
<p><a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.elementwise" title="chainer.cuda.elementwise"><code class="xref py py-func docutils literal"><span class="pre">cuda.elementwise()</span></code></a> function accepts the essential implementation of the kernel function, and returns a kernel invocation function (actually, it returns <code class="xref py py-class docutils literal"><span class="pre">ElementwiseKernel</span></code> object, which is callable).
In typical usage, we pass four arguments to this function as follows:</p>
<ol class="arabic simple">
<li>Input argument list. This is a comma-separated string each entry of which consists of a type specification and an argument name.</li>
<li>Output argument list in the same format as the input argument list.</li>
<li>Body of <em>parallel loop</em>. We can use the input/output argument names as an element of these arrays.</li>
<li>Name of the kernel function, which is shown in debuggers and profilers.</li>
</ol>
<p>Above code is not compiled on every forward/backward computation thanks to two caching mechanisms provided by <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.elementwise" title="chainer.cuda.elementwise"><code class="xref py py-func docutils literal"><span class="pre">cuda.elementwise()</span></code></a>.</p>
<p>The first one is <em>binary caching</em>:
<a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.elementwise" title="chainer.cuda.elementwise"><code class="xref py py-func docutils literal"><span class="pre">cuda.elementwise()</span></code></a> function caches the compiled binary in the <code class="docutils literal"><span class="pre">$(HOME)/.cupy/kernel_cache</span></code> directory with a hash value of the CUDA code, and reuses it if the given code matches the hash value.
This caching mechanism is actually implemented in CuPy.</p>
<p>The second one is <em>upload caching</em>:
Given a compiled binary code, we have to upload it to the current GPU in order to execute it.
<a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.elementwise" title="chainer.cuda.elementwise"><code class="xref py py-func docutils literal"><span class="pre">cuda.elementwise()</span></code></a> function memoizes the arguments and the current device, and if it is called with the same arguments for the same device, it reuses the previously uploaded kernel code.</p>
<p>The above MulAdd code only works for float32 arrays.
The <code class="xref py py-class docutils literal"><span class="pre">ElementwiseKernel</span></code> also supports the type-variadic kernel definition.
In order to define variadic kernel functions, you can use <em>type placeholder</em> by placing a single character as type specifier:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">forward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cupy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">cupy</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">elementwise</span><span class="p">(</span>
            <span class="s1">&#39;T x, T y, T z&#39;</span><span class="p">,</span>
            <span class="s1">&#39;T w&#39;</span><span class="p">,</span>
            <span class="s1">&#39;w = x * y + z&#39;</span><span class="p">,</span>
            <span class="s1">&#39;muladd_fwd&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward_gpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gw</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">elementwise</span><span class="p">(</span>
            <span class="s1">&#39;T x, T y, T gw&#39;</span><span class="p">,</span>
            <span class="s1">&#39;T gx, T gy&#39;</span><span class="p">,</span>
            <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">               gx = y * gw;</span>
<span class="sd">               gy = x * gw;</span>
<span class="sd">            &#39;&#39;&#39;</span><span class="p">,</span>
            <span class="s1">&#39;muladd_bwd&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gw</span><span class="p">)</span>

        <span class="n">gz</span> <span class="o">=</span> <span class="n">gw</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
</pre></div>
</div>
<p>The type placeholder <code class="docutils literal"><span class="pre">T</span></code> indicates an arbitrary data type that CuPy supports.</p>
<p>There are more functionalities on user-defined kernels in CuPy.
<span class="xref std std-ref">See the CuPy documentation on user-defined kernels for more details.</span></p>
</div>
<div class="section" id="write-a-function-with-training-test-mode">
<h2>Write a function with training/test mode<a class="headerlink" href="#write-a-function-with-training-test-mode" title="Permalink to this headline">¶</a></h2>
<p>We sometimes want to make a function behave differently in training and test modes.
The training/test mode in Chainer is configured by <a class="reference internal" href="../reference/core/configuration.html#chainer.config" title="chainer.config"><code class="xref py py-data docutils literal"><span class="pre">chainer.config</span></code></a>.
This is a thread-local configuration object, and users can substitute True or False to its <code class="docutils literal"><span class="pre">train</span></code> attribute.
You can refer to <a class="reference internal" href="../reference/core/configuration.html#configuration"><span class="std std-ref">Configuring Chainer</span></a> to see how to configure this flag as well as other configuration items.</p>
<p>Here, we just show how to use this flag to make a function support training/test mode.
You will need to check the value of the boolean flag <code class="docutils literal"><span class="pre">chainer.config.train</span></code> and branch appropriately.</p>
<p>For example, consider the following simple dropout function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">xp</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_array_module</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask</span>
</pre></div>
</div>
<p>This function applies dropout to each element and doubles survived elemenets to preserve the scale.
The above implementation applies dropout even in test mode, but it is not a desired behavior.
We can fix it as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">chainer</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">train</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="n">xp</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_array_module</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">mask</span>
</pre></div>
</div>
<p>The function now supports test mode.
Note that you usually do not have to implement your own dropout function because <a class="reference internal" href="../reference/functions.html#chainer.functions.dropout" title="chainer.functions.dropout"><code class="xref py py-func docutils literal"><span class="pre">dropout()</span></code></a> is officially provided.</p>
</div>
<div class="section" id="links-that-wrap-functions">
<h2>Links that wrap functions<a class="headerlink" href="#links-that-wrap-functions" title="Permalink to this headline">¶</a></h2>
<p>Some functions are meant to be combined with parameters.
In such case, it is useful to write a small <strong>link</strong> that wraps the function.
We have already seen how to define a chain that wraps other links (by inheriting <a class="reference internal" href="../reference/core/link.html#chainer.Chain" title="chainer.Chain"><code class="xref py py-class docutils literal"><span class="pre">Chain</span></code></a> class).
Here we study how to define a link that does not hold any other links.</p>
<p>As the first example, suppose that we want to implement elementwise product function between the input array and the parameter array.
It can be defined as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EltwiseParamProduct</span><span class="p">(</span><span class="n">Link</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="c1"># By passing a shape of the parameter, the initializer allocates a</span>
        <span class="c1"># parameter variable of the shape.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EltwiseParamProduct</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">W</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p>We can also initialize the parameter after the initialization by the <a class="reference internal" href="../reference/core/link.html#chainer.Link.add_param" title="chainer.Link.add_param"><code class="xref py py-meth docutils literal"><span class="pre">Link.add_param()</span></code></a> method.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EltwiseParamProduct</span><span class="p">(</span><span class="n">Link</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EltwiseParamProduct</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_param</span><span class="p">(</span><span class="s1">&#39;W&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p>Note that the initializer and the <a class="reference internal" href="../reference/core/link.html#chainer.Link.add_param" title="chainer.Link.add_param"><code class="xref py py-meth docutils literal"><span class="pre">add_param()</span></code></a> method does not initialize elements of the parameter array.
We have to manually initialize the elements by random values, zeros, etc.</p>
<p>For another example, assume we want to define a simple linear layer.
It is already defined as <a class="reference internal" href="../reference/links.html#chainer.links.Linear" title="chainer.links.Linear"><code class="xref py py-class docutils literal"><span class="pre">Linear</span></code></a>, so this is an educational example.
The linear layer is divided into two parts: a function and its wrapper link.
First, we have to define a function on variables:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">,</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">grad_outputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="n">gy</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_outputs</span>

        <span class="n">gx</span> <span class="o">=</span> <span class="n">gy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">gW</span> <span class="o">=</span> <span class="n">gy</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">gb</span> <span class="o">=</span> <span class="n">gy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gW</span><span class="p">,</span> <span class="n">gb</span>

<span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">LinearFunction</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>This function takes three arguments: input, weight, and bias.
It can be used as a part of model definition, though is inconvenient since the user have to manage the weight and bias parameters directly.
In order to make a convenient module, let’s wrap it into a link:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">Link</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">W</span><span class="o">=</span><span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">),</span> <span class="n">b</span><span class="o">=</span><span class="n">out_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">in_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>This link hides the parameters of the linear layer.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>An advanced tip to implement functions: if you want to preserve some information between forward and backward computations (e.g. to cache some arrays), you can store it as attributes.
Be careful that it might increase the memory consumption during the whole forward-backward computation.
If you want to train very large networks on a GPU with limited memory, it is not recommended to cache arrays between forward and backward.
There is one exception for this: caching the output arrays does not change the memory consumption, because they are also held by the output Variable objects.</p>
<div class="last admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">You should not assume a one-to-one match of calls of forward and backward.
Some users may call backward more than once after one forward call.</p>
</div>
</div>
</div>
<div class="section" id="testing-function">
<h2>Testing Function<a class="headerlink" href="#testing-function" title="Permalink to this headline">¶</a></h2>
<p>In order to isolate the cause of learning failure from implementation bugs, it is important to test function implementations.
Chainer provides simple utilities to help writing unit tests.
They are defined in the <a class="reference internal" href="../reference/check.html#module-chainer.gradient_check" title="chainer.gradient_check"><code class="xref py py-mod docutils literal"><span class="pre">gradient_check</span></code></a> module.</p>
<p>The most important test utility is the <a class="reference internal" href="../reference/check.html#chainer.gradient_check.numerical_grad" title="chainer.gradient_check.numerical_grad"><code class="xref py py-func docutils literal"><span class="pre">numerical_grad()</span></code></a> function.
This function computes the numerical gradient of given function using finite differences.
It can be used as follows</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">gy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">f</span>  <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">,)</span>
<span class="n">gx</span> <span class="o">=</span> <span class="n">gradient_check</span><span class="o">.</span><span class="n">numerical_grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="n">gy</span><span class="p">,))</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">f</span></code> is a closure that returns a tuple of array(s) computed from input arrays.
The second and third arguments of <a class="reference internal" href="../reference/check.html#chainer.gradient_check.numerical_grad" title="chainer.gradient_check.numerical_grad"><code class="xref py py-func docutils literal"><span class="pre">numerical_grad()</span></code></a> are tuples of input arrays and output gradient arrays, respectively.
The code above computes the numerical gradients of <code class="docutils literal"><span class="pre">sum(f(x))</span></code>, where <code class="docutils literal"><span class="pre">sum</span></code> indicates the summation over all elements.
The summation can be weighted by changing <code class="docutils literal"><span class="pre">gy</span></code>.
<a class="reference internal" href="../reference/check.html#chainer.gradient_check.numerical_grad" title="chainer.gradient_check.numerical_grad"><code class="xref py py-func docutils literal"><span class="pre">numerical_grad()</span></code></a> function also accepts additional <code class="docutils literal"><span class="pre">eps</span></code> argument, which indicates the quantization width of finite differences.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><a class="reference internal" href="../reference/check.html#chainer.gradient_check.numerical_grad" title="chainer.gradient_check.numerical_grad"><code class="xref py py-func docutils literal"><span class="pre">numerical_grad()</span></code></a> function accepts both CPU and GPU arrays.
Note that we cannot mix CPU and GPU arrays.</p>
</div>
<p>Another utility is <a class="reference internal" href="../reference/check.html#chainer.testing.assert_allclose" title="chainer.testing.assert_allclose"><code class="xref py py-func docutils literal"><span class="pre">chainer.testing.assert_allclose()</span></code></a> function.
This is similar to <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.testing.assert_allclose.html#numpy.testing.assert_allclose" title="(in NumPy v1.13)"><code class="xref py py-func docutils literal"><span class="pre">numpy.testing.assert_allclose()</span></code></a> function.
The difference is that Chainer’s version accepts CPU and GPU arrays as inputs.
We can mix them in one invocation of <a class="reference internal" href="../reference/check.html#chainer.testing.assert_allclose" title="chainer.testing.assert_allclose"><code class="xref py py-func docutils literal"><span class="pre">chainer.testing.assert_allclose()</span></code></a>.
The default values of optional arguments are also different.</p>
<p>Here is a typical usage of gradient checking utilities.
This is a test example of <a class="reference internal" href="../reference/functions.html#chainer.functions.relu" title="chainer.functions.relu"><code class="xref py py-func docutils literal"><span class="pre">functions.relu()</span></code></a> function</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">unittest</span>

<span class="kn">from</span> <span class="nn">chainer</span> <span class="k">import</span> <span class="n">testing</span>

<span class="k">class</span> <span class="nc">TestReLU</span><span class="p">(</span><span class="n">unittest</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_backward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">data</span><span class="p">,)</span>
        <span class="n">gx</span><span class="p">,</span> <span class="o">=</span> <span class="n">gradient_check</span><span class="o">.</span><span class="n">numerical_grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,),</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">,))</span>

        <span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">gx</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p>The first four lines of the test code are simple forward and backward computation of ReLU function.
The next two lines compute numerical gradient using the same forward function without backward routine.
And at last, we compare these two results elementwise.
Note that the above test code can be easily modified to test GPU version just by replacing CPU arrays to GPU arrays.</p>
<p>You can find many examples of function tests under <code class="docutils literal"><span class="pre">tests/chainer_tests/function_tests</span></code> directory.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="type_check.html" class="btn btn-neutral float-right" title="Type check" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpu.html" class="btn btn-neutral" title="Using GPU(s) in Chainer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Preferred Networks, inc. and Preferred Infrastructure, inc..

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'2.0.0b1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>