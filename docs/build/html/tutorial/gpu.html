

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using GPU(s) in Chainer &mdash; Chainer 2.0.0b1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/modified_theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Chainer 2.0.0b1 documentation" href="../index.html"/>
        <link rel="up" title="Chainer Tutorial" href="index.html"/>
        <link rel="next" title="Define your own function" href="function.html"/>
        <link rel="prev" title="Recurrent Nets and their Computational Graph" href="recurrentnet.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Chainer
          

          
          </a>

          
            
            
              <div class="version">
                2.0.0b1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Chainer Tutorial</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="basic.html">Introduction to Chainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="recurrentnet.html">Recurrent Nets and their Computational Graph</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using GPU(s) in Chainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#relationship-between-chainer-and-cupy">Relationship between Chainer and CuPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#basics-of-cupy-ndarray">Basics of <code class="docutils literal"><span class="pre">cupy.ndarray</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-neural-networks-on-a-single-gpu">Run Neural Networks on a Single GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-parallel-computation-on-multiple-gpus">Model-parallel Computation on Multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel-computation-on-multiple-gpus-with-trainer">Data-parallel Computation on Multiple GPUs with Trainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel-computation-on-multiple-gpus-without-trainer">Data-parallel Computation on Multiple GPUs without Trainer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="function.html">Define your own function</a></li>
<li class="toctree-l2"><a class="reference internal" href="type_check.html">Type check</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">Chainer Reference Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Chainer Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compatibility.html">API Compatibility Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tips.html">Tips and FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../comparison.html">Comparison with Other Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Chainer</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Chainer Tutorial</a> &raquo;</li>
        
      <li>Using GPU(s) in Chainer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/gpu.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="using-gpu-s-in-chainer">
<h1>Using GPU(s) in Chainer<a class="headerlink" href="#using-gpu-s-in-chainer" title="Permalink to this headline">¶</a></h1>
<p>In this section, you will learn about the following things:</p>
<ul class="simple">
<li>Relationship between Chainer and CuPy</li>
<li>Basics of CuPy</li>
<li>Single-GPU usage of Chainer</li>
<li>Multi-GPU usage of model-parallel computing</li>
<li>Multi-GPU usage of data-parallel computing</li>
</ul>
<p>After reading this section, you will be able to:</p>
<ul class="simple">
<li>Use Chainer on a CUDA-enabled GPU</li>
<li>Write model-parallel computing in Chainer</li>
<li>Write data-parallel computing in Chainer</li>
</ul>
<div class="section" id="relationship-between-chainer-and-cupy">
<h2>Relationship between Chainer and CuPy<a class="headerlink" href="#relationship-between-chainer-and-cupy" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">From v2.0.0, CuPy is turned into a separate package and repository.
Even if you have CUDA installed in your environment, you have to install CuPy separately to use GPUs.
See <a class="reference internal" href="../install.html#install-cuda"><span class="std std-ref">Enable CUDA/cuDNN support</span></a> for the way to set up CUDA support.</p>
</div>
<p>Chainer uses <a class="reference external" href="http://docs.cupy.chainer.org/">CuPy</a> as its backend for GPU computation.
In particular, the <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code> class is the GPU array implementation for Chainer.
CuPy supports a subset of features of NumPy with a compatible interface.
It enables us to write a common code for CPU and GPU.
It also supports PyCUDA-like user-defined kernel generation, which enables us to write fast implementations dedicated to GPU.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <a class="reference internal" href="../reference/util/cuda.html#module-chainer.cuda" title="chainer.cuda"><code class="xref py py-mod docutils literal"><span class="pre">chainer.cuda</span></code></a> module imports many important symbols from CuPy.
For example, the cupy namespace is referred as <code class="docutils literal"><span class="pre">cuda.cupy</span></code> in the Chainer code.
Note that the <a class="reference internal" href="../reference/util/cuda.html#module-chainer.cuda" title="chainer.cuda"><code class="xref py py-mod docutils literal"><span class="pre">chainer.cuda</span></code></a> module can be imported even if CUDA is not installed.</p>
</div>
<p>Chainer uses a memory pool for GPU memory allocation.
As shown in the previous sections, Chainer constructs and destructs many arrays during learning and evaluating iterations.
It is not well suited for CUDA architecture, since memory allocation and release in CUDA (i.e. <code class="docutils literal"><span class="pre">cudaMalloc</span></code> and <code class="docutils literal"><span class="pre">cudaFree</span></code> functions) synchronize CPU and GPU computations, which hurts performance.
In order to avoid memory allocation and deallocation during the computation, Chainer uses CuPy’s memory pool as the standard memory allocator.
Chainer changes the default allocator of CuPy to the memory pool, so user can use functions of CuPy directly without dealing with the memory allocator.</p>
</div>
<div class="section" id="basics-of-cupy-ndarray">
<h2>Basics of <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code><a class="headerlink" href="#basics-of-cupy-ndarray" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">CuPy does not require explicit initialization, so <code class="docutils literal"><span class="pre">cuda.init()</span></code> function is deprecated.</p>
</div>
<p>CuPy is a GPU array backend that implements a subset of NumPy interface.
The <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code> class is in its core, which is a compatible GPU alternative of <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a>.
CuPy implements many functions on <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code> objects.
<span class="xref std std-ref">See the reference for the supported subset of NumPy API</span>.
Understanding NumPy might help utilizing most features of CuPy.
<a class="reference external" href="http://docs.scipy.org/doc/numpy/index.html">See the NumPy documentation for learning it</a>.</p>
<p>The main difference of <code class="xref py py-class docutils literal"><span class="pre">cupy.ndarray</span></code> from <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a> is that the content is allocated on the device memory.
The allocation takes place on the current device by default.
The current device can be changed by <code class="xref py py-class docutils literal"><span class="pre">cupy.cuda.Device</span></code> object as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">cupy</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x_on_gpu1</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>Most operations of CuPy is done on the current device.
Be careful that it causes an error to process an array on a non-current device.</p>
<p>Chainer provides some convenient functions to automatically switch and choose the device.
For example, the <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.to_gpu" title="chainer.cuda.to_gpu"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.to_gpu()</span></code></a> function copies a <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><code class="xref py py-class docutils literal"><span class="pre">numpy.ndarray</span></code></a> object to a specified device:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_gpu</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">x_cpu</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>It is equivalent to the following code using CuPy:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">with</span> <span class="n">cupy</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x_gpu</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_cpu</span><span class="p">)</span>
</pre></div>
</div>
<p>Moving a device array to the host can be done by <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.to_cpu" title="chainer.cuda.to_cpu"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.to_cpu()</span></code></a> as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">x_cpu</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">(</span><span class="n">x_gpu</span><span class="p">)</span>
</pre></div>
</div>
<p>It is equivalent to the following code using CuPy:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
    <span class="n">x_cpu</span> <span class="o">=</span> <span class="n">x_gpu</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <em>with</em> statements in these codes are required to select the appropriate CUDA device.
If user uses only one device, these device switching is not needed.
<a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.to_cpu" title="chainer.cuda.to_cpu"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.to_cpu()</span></code></a> and <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.to_gpu" title="chainer.cuda.to_gpu"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.to_gpu()</span></code></a> functions automatically switch the current device correctly.</p>
</div>
<p>Chainer also provides a convenient function <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.get_device" title="chainer.cuda.get_device"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.get_device()</span></code></a> to select a device.
It accepts an integer, CuPy array, NumPy array, or None (indicating the current device), and returns an appropriate device object.
If the argument is a NumPy array, then <em>a dummy device object</em> is returned.
The dummy device object supports <em>with</em> statements like above which does nothing.
Here are some examples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">use</span><span class="p">()</span>
<span class="n">x_gpu1</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>  <span class="c1"># &#39;f&#39; indicates float32</span>

<span class="k">with</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x_gpu1</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="n">x_gpu1</span><span class="p">):</span>
    <span class="n">y_gpu1</span> <span class="o">=</span> <span class="n">x_gpu</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Since it accepts NumPy arrays, we can write a function that accepts both NumPy and CuPy arrays with correct device switching:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_device</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The compatibility of CuPy with NumPy enables us to write CPU/GPU generic code.
It can be made easy by the <a class="reference internal" href="../reference/util/cuda.html#chainer.cuda.get_array_module" title="chainer.cuda.get_array_module"><code class="xref py py-func docutils literal"><span class="pre">chainer.cuda.get_array_module()</span></code></a> function.
This function returns the <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/index.html#module-numpy" title="(in NumPy v1.13)"><code class="xref py py-mod docutils literal"><span class="pre">numpy</span></code></a> or <code class="xref py py-mod docutils literal"><span class="pre">cupy</span></code> module based on arguments.
A CPU/GPU generic function is defined using it like follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># Stable implementation of log(1 + exp(x))</span>
<span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">xp</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">get_array_module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">xp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">xp</span><span class="o">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">xp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="section" id="run-neural-networks-on-a-single-gpu">
<h2>Run Neural Networks on a Single GPU<a class="headerlink" href="#run-neural-networks-on-a-single-gpu" title="Permalink to this headline">¶</a></h2>
<p>Single-GPU usage is very simple.
What you have to do is transferring <a class="reference internal" href="../reference/core/link.html#chainer.Link" title="chainer.Link"><code class="xref py py-class docutils literal"><span class="pre">Link</span></code></a> and input arrays to the GPU beforehand.
In this subsection, the code is based on <a class="reference internal" href="basic.html#mnist-mlp-example"><span class="std std-ref">our first MNIST example in this tutorial</span></a>.</p>
<p>A <a class="reference internal" href="../reference/core/link.html#chainer.Link" title="chainer.Link"><code class="xref py py-class docutils literal"><span class="pre">Link</span></code></a> object can be transferred to the specified GPU using the <a class="reference internal" href="../reference/core/link.html#chainer.Link.to_gpu" title="chainer.Link.to_gpu"><code class="xref py py-meth docutils literal"><span class="pre">to_gpu()</span></code></a> method.</p>
<p>This time, we make the number of input, hidden, and output units configurable.
The <a class="reference internal" href="../reference/core/link.html#chainer.Link.to_gpu" title="chainer.Link.to_gpu"><code class="xref py py-meth docutils literal"><span class="pre">to_gpu()</span></code></a> method also accepts a device ID like <code class="docutils literal"><span class="pre">model.to_gpu(0)</span></code>.
In this case, the link object is transferred to the appropriate GPU device.
The current device is used by default.</p>
<p>If we use <a class="reference internal" href="../reference/core/training.html#chainer.training.Trainer" title="chainer.training.Trainer"><code class="xref py py-class docutils literal"><span class="pre">chainer.training.Trainer</span></code></a>, what we have to do is just let the updater know the device ID to send each mini-batch.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">updater</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">StandardUpdater</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">updater</span><span class="p">,</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">),</span> <span class="n">out</span><span class="o">=</span><span class="s1">&#39;result&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>We also have to specify the device ID for an evaluator extension as well.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extensions</span><span class="o">.</span><span class="n">Evaluator</span><span class="p">(</span><span class="n">test_iter</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>When we write down the training loop by hand, we have to transfer each mini-batch to the GPU manually:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">()</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">datasize</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">datasize</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">datasize</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]]))</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]]))</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="model-parallel-computation-on-multiple-gpus">
<h2>Model-parallel Computation on Multiple GPUs<a class="headerlink" href="#model-parallel-computation-on-multiple-gpus" title="Permalink to this headline">¶</a></h2>
<p>Parallelization of machine learning is roughly classified into two types called “model-parallel” and “data-parallel”.
Model-parallel means parallelizations of the computations inside the model.
In contrast, data-parallel means parallelizations using data sharding.
In this subsection, we show how to use the model-parallel approach on multiple GPUs in Chainer.</p>
<p><a class="reference internal" href="basic.html#mnist-mlp-example"><span class="std std-ref">Recall the MNIST example</span></a>.
Now suppose that we want to modify this example by expanding the network to 6 layers with 2000 units each using two GPUs.
In order to make multi-GPU computation efficient, we only make the two GPUs communicate at the third and sixth layer.
The overall architecture looks like the following diagram:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">GPU0</span><span class="p">)</span> <span class="nb">input</span> <span class="o">--+--&gt;</span> <span class="n">l1</span> <span class="o">--&gt;</span> <span class="n">l2</span> <span class="o">--&gt;</span> <span class="n">l3</span> <span class="o">--+--&gt;</span> <span class="n">l4</span> <span class="o">--&gt;</span> <span class="n">l5</span> <span class="o">--&gt;</span> <span class="n">l6</span> <span class="o">--+--&gt;</span> <span class="n">output</span>
               <span class="o">|</span>                       <span class="o">|</span>                       <span class="o">|</span>
<span class="p">(</span><span class="n">GPU1</span><span class="p">)</span>         <span class="o">+--&gt;</span> <span class="n">l1</span> <span class="o">--&gt;</span> <span class="n">l2</span> <span class="o">--&gt;</span> <span class="n">l3</span> <span class="o">--+--&gt;</span> <span class="n">l4</span> <span class="o">--&gt;</span> <span class="n">l5</span> <span class="o">--&gt;</span> <span class="n">l6</span> <span class="o">--+</span>
</pre></div>
</div>
<p>We can use the above MLP chain as following diagram:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">GPU0</span><span class="p">)</span> <span class="nb">input</span> <span class="o">--+--&gt;</span> <span class="n">mlp1</span> <span class="o">--+--&gt;</span> <span class="n">mlp2</span> <span class="o">--+--&gt;</span> <span class="n">output</span>
               <span class="o">|</span>           <span class="o">|</span>           <span class="o">|</span>
<span class="p">(</span><span class="n">GPU1</span><span class="p">)</span>         <span class="o">+--&gt;</span> <span class="n">mlp1</span> <span class="o">--+--&gt;</span> <span class="n">mlp2</span> <span class="o">--+</span>
</pre></div>
</div>
<p>Let’s write a link for the whole network.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ParallelMLP</span><span class="p">(</span><span class="n">Chain</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ParallelMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="c1"># the input size, 784, is inferred</span>
            <span class="n">mlp1_gpu0</span><span class="o">=</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">mlp1_gpu1</span><span class="o">=</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>

            <span class="c1"># the input size, 2000, is inferred</span>
            <span class="n">mlp2_gpu0</span><span class="o">=</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">mlp2_gpu1</span><span class="o">=</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># assume x is on GPU 0</span>
        <span class="n">z0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp1_gpu0</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp1_gpu1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># sync</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">z0</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">z1</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">z0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">y0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp2_gpu0</span><span class="p">(</span><span class="n">h0</span><span class="p">)</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp2_gpu1</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>

        <span class="c1"># sync</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y0</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>  <span class="c1"># output is on GPU0</span>
</pre></div>
</div>
<p>Recall that the <a class="reference internal" href="../reference/core/link.html#chainer.Link.to_gpu" title="chainer.Link.to_gpu"><code class="xref py py-meth docutils literal"><span class="pre">Link.to_gpu()</span></code></a> method returns the link itself.
The <a class="reference internal" href="../reference/functions.html#chainer.functions.copy" title="chainer.functions.copy"><code class="xref py py-func docutils literal"><span class="pre">copy()</span></code></a> function copies an input variable to specified GPU device and returns a new variable on the device.
The copy supports backprop, which just reversely transfers an output gradient to the input device.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Above code is not parallelized on CPU, but is parallelized on GPU.
This is because all the functions in the above code run asynchronously to the host CPU.</p>
</div>
<p>An almost identical example code can be found at <a class="reference external" href="https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist_model_parallel.py">examples/mnist/train_mnist_model_parallel.py</a>.</p>
</div>
<div class="section" id="data-parallel-computation-on-multiple-gpus-with-trainer">
<h2>Data-parallel Computation on Multiple GPUs with Trainer<a class="headerlink" href="#data-parallel-computation-on-multiple-gpus-with-trainer" title="Permalink to this headline">¶</a></h2>
<p>Data-parallel computation is another strategy to parallelize online processing.
In the context of neural networks, it means that a different device does computation on a different subset of the input data.
In this subsection, we review the way to achieve data-parallel learning on two GPUs.</p>
<p>Suppose again our task is <a class="reference internal" href="basic.html#mnist-mlp-example"><span class="std std-ref">the MNIST example</span></a>.
This time we want to directly parallelize the three-layer network.
The most simple form of data-parallelization is parallelizing the gradient computation for a distinct set of data.
First, define a model and optimizer instances:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Classifier</span><span class="p">(</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># the input size, 784, is inferred</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Recall that the <code class="docutils literal"><span class="pre">MLP</span></code> link implements the multi-layer perceptron, and the <a class="reference internal" href="../reference/links.html#chainer.links.Classifier" title="chainer.links.Classifier"><code class="xref py py-class docutils literal"><span class="pre">Classifier</span></code></a> link wraps it to provide a classifier interface.
We used <a class="reference internal" href="../reference/core/training.html#chainer.training.StandardUpdater" title="chainer.training.StandardUpdater"><code class="xref py py-class docutils literal"><span class="pre">StandardUpdater</span></code></a> in the previous example.
In order to enable data-parallel computation with multiple GPUs, we only have to replace it with <a class="reference internal" href="../reference/core/training.html#chainer.training.ParallelUpdater" title="chainer.training.ParallelUpdater"><code class="xref py py-class docutils literal"><span class="pre">ParallelUpdater</span></code></a>.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">updater</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">ParallelUpdater</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span>
                                   <span class="n">devices</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;main&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">devices</span></code> option specifies which devices to use in data-parallel learning.
The device with name <code class="docutils literal"><span class="pre">'main'</span></code> is used as the main device.
The original model is sent to this device, so the optimization runs on the main device.
In the above example, the model is also cloned and sent to GPU 1.
Half of each mini-batch is fed to this cloned model.
After every backward computation, the gradient is accumulated into the main device, the parameter update runs on it, and then the updated parameters are sent to GPU 1 again.</p>
<p>See also the example code in <a class="reference external" href="https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist_data_parallel.py">examples/mnist/train_mnist_data_parallel.py</a>.</p>
</div>
<div class="section" id="data-parallel-computation-on-multiple-gpus-without-trainer">
<h2>Data-parallel Computation on Multiple GPUs without Trainer<a class="headerlink" href="#data-parallel-computation-on-multiple-gpus-without-trainer" title="Permalink to this headline">¶</a></h2>
<p>We here introduce a way to write data-parallel computation without the help of <a class="reference internal" href="../reference/core/training.html#chainer.training.Trainer" title="chainer.training.Trainer"><code class="xref py py-class docutils literal"><span class="pre">Trainer</span></code></a>.
Most users can skip this section.
If you are interested in how to write a data-parallel computation by yourself, this section should be informative.
It is also helpful to, e.g., customize the <a class="reference internal" href="../reference/core/training.html#chainer.training.ParallelUpdater" title="chainer.training.ParallelUpdater"><code class="xref py py-class docutils literal"><span class="pre">ParallelUpdater</span></code></a> class.</p>
<p>We again start from the MNIST example.
At this time, we use a suffix like <code class="docutils literal"><span class="pre">_0</span></code> and <code class="docutils literal"><span class="pre">_1</span></code> to distinguish objects on each device.
First, we define a model.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">model_0</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Classifier</span><span class="p">(</span><span class="n">MLP</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># the input size, 784, is inferred</span>
</pre></div>
</div>
<p>We want to make two copies of this instance on different GPUs.
The <a class="reference internal" href="../reference/core/link.html#chainer.Link.to_gpu" title="chainer.Link.to_gpu"><code class="xref py py-meth docutils literal"><span class="pre">Link.to_gpu()</span></code></a> method runs in place, so we cannot use it to make a copy.
In order to make a copy, we can use <a class="reference internal" href="../reference/core/link.html#chainer.Link.copy" title="chainer.Link.copy"><code class="xref py py-meth docutils literal"><span class="pre">Link.copy()</span></code></a> method.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="n">model_1</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model_0</span><span class="p">)</span>
<span class="n">model_0</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model_1</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="../reference/core/link.html#chainer.Link.copy" title="chainer.Link.copy"><code class="xref py py-meth docutils literal"><span class="pre">Link.copy()</span></code></a> method copies the link into another instance.
<em>It just copies the link hierarchy</em>, and does not copy the arrays it holds.</p>
<p>Then, set up an optimizer:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model_0</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we use the first copy of the model as <em>the master model</em>.
Before its update, gradients of <code class="docutils literal"><span class="pre">model_1</span></code> must be aggregated to those of <code class="docutils literal"><span class="pre">model_0</span></code>.</p>
<p>Then, we can write a data-parallel learning loop as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">batchsize</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">datasize</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;epoch </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">epoch</span><span class="p">)</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">datasize</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">datasize</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">indexes</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">batchsize</span><span class="p">]]</span>

        <span class="n">x0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[:</span><span class="n">batchsize</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">y_batch</span><span class="p">[:</span><span class="n">batchsize</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">x_batch</span><span class="p">[</span><span class="n">batchsize</span><span class="o">//</span><span class="mi">2</span><span class="p">:],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_gpu</span><span class="p">(</span><span class="n">y_batch</span><span class="p">[</span><span class="n">batchsize</span><span class="o">//</span><span class="mi">2</span><span class="p">:],</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">loss_0</span> <span class="o">=</span> <span class="n">model_0</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">t0</span><span class="p">)</span>
        <span class="n">loss_1</span> <span class="o">=</span> <span class="n">model_1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">t1</span><span class="p">)</span>

        <span class="n">model_0</span><span class="o">.</span><span class="n">cleargrads</span><span class="p">()</span>
        <span class="n">model_1</span><span class="o">.</span><span class="n">cleargrads</span><span class="p">()</span>

        <span class="n">loss_0</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">loss_1</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">model_0</span><span class="o">.</span><span class="n">addgrads</span><span class="p">(</span><span class="n">model_1</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

        <span class="n">model_1</span><span class="o">.</span><span class="n">copyparams</span><span class="p">(</span><span class="n">model_0</span><span class="p">)</span>
</pre></div>
</div>
<p>Do not forget to clear the gradients of both model copies!
One half of the mini-batch is forwarded to GPU 0, the other half to GPU 1.
Then the gradients are accumulated by the <a class="reference internal" href="../reference/core/link.html#chainer.Link.addgrads" title="chainer.Link.addgrads"><code class="xref py py-meth docutils literal"><span class="pre">Link.addgrads()</span></code></a> method.
This method adds the gradients of a given link to those of the self.
After the gradients are prepared, we can update the optimizer in usual way.
Note that the update only modifies the parameters of <code class="docutils literal"><span class="pre">model_0</span></code>.
So we must manually copy them to <code class="docutils literal"><span class="pre">model_1</span></code> using <a class="reference internal" href="../reference/core/link.html#chainer.Link.copyparams" title="chainer.Link.copyparams"><code class="xref py py-meth docutils literal"><span class="pre">Link.copyparams()</span></code></a> method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the batch size used in one model remain the same, the scale of the gradient
is roughly proportional to the number of models, when we aggregate
gradients from all models by <a class="reference internal" href="../reference/core/link.html#chainer.Link.addgrads" title="chainer.Link.addgrads"><code class="xref py py-func docutils literal"><span class="pre">chainer.Link.addgrads()</span></code></a>. So you need to adjust the batch size
and/or learning rate of the optimizer accordingly.</p>
</div>
<hr class="docutils" />
<p>Now you can use Chainer with GPUs.
All examples in the <code class="docutils literal"><span class="pre">examples</span></code> directory support GPU computation, so please refer to them if you want to know more practices on using GPUs.
In the next section, we will show how to define a differentiable (i.e. <em>backpropable</em>) function on Variable objects.
We will also show there how to write a simple (elementwise) CUDA kernel using Chainer’s CUDA utilities.</p>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="function.html" class="btn btn-neutral float-right" title="Define your own function" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="recurrentnet.html" class="btn btn-neutral" title="Recurrent Nets and their Computational Graph" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Preferred Networks, inc. and Preferred Infrastructure, inc..

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'2.0.0b1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>